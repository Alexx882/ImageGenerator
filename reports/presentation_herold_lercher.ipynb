{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generating Images of Faces with Generative Adversarial Networks\n",
    "by Manuel Herold and Alexander Lercher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "- Project Overview\n",
    "- Training Data\n",
    "\n",
    "- GAN Theory\n",
    "- Chosen GAN Architectures (Lercher)\n",
    "- Selected Implementation Details (Lercher)\n",
    "- Results (Lercher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Agenda /2\n",
    "- VAE Theory\n",
    "- Chosen GAN Architectures (Herold)\n",
    "- Selected Implementation Details (Herold)\n",
    "- Results (Herold)\n",
    "\n",
    "- Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Project Overview\n",
    "\n",
    "- Generation of Images of Human Faces\n",
    "- Application of Generative Adversarial Network (GAN)\n",
    "    - Generator: Image Generation\n",
    "    - Discriminator: Classification as Real or Generated\n",
    "- Selection of Published GAN Architectures\n",
    "- Implementation in Python / TensorFlow\n",
    "- Evaluation of Training Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Task Distribution\n",
    "| Task                       | Team member       | Description                     |\n",
    "|:---------------------------|:------------------|:--------------------------------|\n",
    "| Training Data Collection   | Manuel; Alexander | Collection of raw images        |\n",
    "| Training Data Alignment    | Manuel            | Preparation of images for learning      |\n",
    "| GAN Training Pipeline      | Alexander         | Training process implementation |\n",
    "| GAN Reference Architecture | Alexander         | Image generation from [1, 2]       |\n",
    "| GAN VAE Noise Input        | Manuel            | Image generation from [3]       |\n",
    "\n",
    "[1] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks; Alec Radford, Luke Metz, Soumith Chintala\n",
    "\n",
    "[2] High-resolution Deep Convolutional Generative Adversarial Networks; J. D. Curt√≥, I. C. Zarza, Fernando de la Torre, Irwin King, Michael R. Lyu\n",
    "\n",
    "[3] Generative Adversarial Networks with Decoder-Encoder Output Noise; Guoqiang Zhong, Wei Gao, Yongbin Liu, Youzhao Yang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Data\n",
    "- Images of real faces, so the GAN can learn features from them\n",
    "\n",
    "\n",
    "- Images crawled from model websites\n",
    "- Images from *Labeled Faces in the Wild*\n",
    "- Images from *CelebA*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### CelebA Dataset\n",
    "<img src=\"http://mmlab.ie.cuhk.edu.hk/projects/CelebA/overview.png\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "- Images are converted to greyscale (1 channel)\n",
    "- Image dimensions are cropped to our desired size (256x256)\n",
    "- Faces are moved to the middle of the cropped image\n",
    "\n",
    "\n",
    "- Prepared images are combined to batches of size 64\n",
    "- Batches are stored on disk as numpy arrays (64, 256, 256, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reasons for Additional Preparation\n",
    "- All images without detected face were removed\n",
    "- Therefore, all training images contain a face in the center\n",
    "\n",
    "\n",
    "- GAN learns to focus on center \n",
    "- Background is less important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Adversarial Networks\n",
    "\n",
    "<img src=\"https://developers.google.com/machine-learning/gan/images/gan_diagram.svg\" width=\"80%\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Discriminator Goal\n",
    "- Classify input images into two classes:\n",
    "    - fake / generated \n",
    "    - real \n",
    "\n",
    "- Loss function:\n",
    "     - $ L(D) = -\\frac{1}{2} \\mathbb{E}_x [ log (D(x)) ] -\\frac{1}{2} \\mathbb{E}_z [ log (1 - D(G(z))) ] $\n",
    "\n",
    "- Theoretical optimum:\n",
    "    - $D(x)=1$\n",
    "    - $D(G(z))=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Intuitively, the first part optimizes predictions for real data $x$ which should be classified as real, with $D(x)=1$. The second part represents predictions for fake data from the generator which should be classified as fake, with $D(G(z))=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generator Goal\n",
    "- Generation of realistic images\n",
    "    - based on real training images\n",
    "    - should fool discriminator\n",
    "\n",
    "- Loss function:\n",
    "    - $ L(G) = - \\mathbb{E}_z [ log (D(G(z))) ] $\n",
    "\n",
    "- Theoretical optimum:\n",
    "    - $D(G(z))=1$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convolutions\n",
    "The implemented networks employ two-dimensional convolutions\n",
    "\n",
    "Discriminator:\n",
    "- Image as input, eg. (256, 256, 1)\n",
    "- Convolutions with stride 2 (downsampling)\n",
    "- Binary classification result as output, eg. real or not real\n",
    "\n",
    "Generator:\n",
    "- Noise as input, eg. random vector of size 100\n",
    "- Transposed convolutions with stride 2 (upsampling)\n",
    "- Image as output, eg. (256, 256, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The ANNs have to learn the actual values for the convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convolution\n",
    "- Kernel size = 3\n",
    "- Stride = 2\n",
    "\n",
    "<img alt=\"2D Convolution Stride 2\"\n",
    "     src=\"https://miro.medium.com/max/294/1*BMngs93_rm2_BpJFH2mS0Q.gif\" \n",
    "     width=\"200\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transposed Convolution\n",
    "- Kernel size = 3\n",
    "- Stride = 2\n",
    "\n",
    "<img alt=\"2D Transposed Convolution Stride 2\"\n",
    "     src=\"https://miro.medium.com/max/395/1*Lpn4nag_KRMfGkx1k6bV-g.gif\" \n",
    "     width=\"200\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chosen GAN Architectures (Lercher)\n",
    "- Deep Convolutional GAN (DCGAN)\n",
    "- High-Resolution Deep Convolutional GAN (HR-DCGAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Deep Convolutional GAN (DCGAN)\n",
    "- Chains multiple convolutions with stride 2\n",
    "- Each convolution doubles/ halves the resulting matrix size\n",
    "- Batch normalization for every layer\n",
    "    - Except generator output and discriminator input\n",
    "    - BatchNorm normalizes the values \n",
    "    - Helps with gradient flow\n",
    "- Linear activation functions\n",
    "    - ReLU for generator\n",
    "    - LeakyReLU for discriminator\n",
    "- Adam optimizer with learning rate 0.0002 and momentum 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"lercher/dcgan_generator_architecture.png\" width=\"80%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### High-Resolution Deep Convolutional GAN (HR-DCGAN)\n",
    "- Improvement of DCGAN\n",
    "- Almost same convolution clain\n",
    "    - two new layers with stride 1\n",
    "    - first for generator \n",
    "    - last for discriminator\n",
    "- Exponential activation functions\n",
    "    - SELU for generator and discriminator\n",
    "- Same optimizer and hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img alt=\"HR-DCGAN Architecture\"\n",
    "     src=\"lercher/hrdcgan_architecture.png\" \n",
    "     height=\"90%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selected Implementation Details (Lercher)\n",
    "- Implementation was done in Python 3 with TensorFlow\n",
    "- Object-oriented implementation for better maintainability\n",
    "- Jupyter Notebooks for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training Data Preprocessing\n",
    "- Loads all image paths from disk\n",
    "- Preprocesses individual images as explained previously\n",
    "- Combines them to batches and yields every batch to the caller directly\n",
    "\n",
    "Pro:\n",
    "- Only a subset of training images is loaded into memory\n",
    "\n",
    "Con:\n",
    "- The processing has to be done for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_training_images_in_batches(self) -> Iterable[np.ndarray]:\n",
    "    '''\n",
    "    Preprocesses all training images and returns them in multiple batches, where one batch is one numpy array.\n",
    "\n",
    "    :returns: arrays with shape (self.batch_size, self.image_width, self.image_height, 1)\n",
    "    '''\n",
    "    current_batch = [] # the current batch to fill\n",
    "\n",
    "    for image_path in self._load_all_images_from_disk():\n",
    "        image: Image = Image.open(image_path)\n",
    "        try:\n",
    "            image = self._preprocess_image(image)\n",
    "        except LookupError:\n",
    "            continue # no face was found in image\n",
    "\n",
    "        current_batch.append(np.array(image))\n",
    "\n",
    "        if len(current_batch) == self.batch_size:\n",
    "            yield self._convert_image_batch_to_training_array(current_batch)\n",
    "            current_batch = []\n",
    "\n",
    "    if len(current_batch) > 0:\n",
    "        yield self._convert_image_batch_to_training_array(current_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training Data Preparation /2\n",
    "- Preprocessed data is stored to disk once\n",
    "- Then loaded on every epoch\n",
    "\n",
    "Pro:\n",
    "- Faster training as images are only processed once\n",
    "\n",
    "Con:\n",
    "- Prepared images have to be stored on disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_training_images_in_batches_from_disk(self) -> Iterable[np.ndarray]:\n",
    "    '''\n",
    "    Loads preprocessed image arrays from files. \n",
    "    Memory load is not that high, as only individual batches are read in.\n",
    "    The batch size is fixed to 64.\n",
    "\n",
    "    :returns: same as self.get_all_training_images_in_batches() but faster.\n",
    "    '''\n",
    "    if not os.path.exists(self.npy_data_path):\n",
    "        raise IOError(f\"The training arrays folder {self.npy_data_path} does not exist.\")\n",
    "\n",
    "    for _, _, files in os.walk(self.npy_data_path):\n",
    "        random.shuffle(files)\n",
    "        for file_ in files:\n",
    "            yield np.load(os.path.join(self.npy_data_path, file_), allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GAN Training Process\n",
    "- Training process is implemented once and used for all GAN architectures\n",
    "- Strategy pattern to add new architectures more easily\n",
    "- A new network only needs to provide:\n",
    "    - new architectures\n",
    "    - noise dimension\n",
    "    - optimizer hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img alt=\"HR-DCGAN Architecture\"\n",
    "     src=\"lercher/gan_strategy_pattern.png\" \n",
    "     width=\"40%\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Implementation of Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def discriminator_loss(real_output, generated_output):\n",
    "    '''The discriminator loss function, where real output should be classified as 1 and generated as 0.'''\n",
    "    return GAN.bce(tf.ones_like(real_output), real_output) + GAN.bce(tf.zeros_like(generated_output), generated_output)\n",
    "\n",
    "@staticmethod\n",
    "def generator_loss(generated_output):\n",
    "    '''The generator loss function, where generated output should be classified as 0.'''\n",
    "    return GAN.bce(tf.ones_like(generated_output), generated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Implementation of Training Steps \n",
    "First approach:\n",
    "- Training of discriminator and generator taking turns\n",
    "- Using build-in TensorFlow training methods\n",
    "- eg. training one ANN for 20 iterations while the other is stable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Implementation of Training Steps /2\n",
    "Second approach:\n",
    "- Training of discriminator and generator in parallel\n",
    "    - Generating the results\n",
    "    - Calculating the losses\n",
    "    - Calculating the gradients\n",
    "    - Updating the weights\n",
    "- Using a TensorFlow function to execute the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(self, real_data_batch: np.ndarray) -> '(disc_loss, gen_loss)':\n",
    "    # prepare real data and noise input\n",
    "    batch_size = real_data_batch.shape[0]\n",
    "    noise = tf.random.normal([batch_size, self.get_noise_dim()])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # Predict images with G\n",
    "        gen_data = self.generator(noise, training=True)\n",
    "\n",
    "        # Predict classes with D\n",
    "        d_real_predicted_labels = self.discriminator(real_data_batch, training=True)\n",
    "        d_fake_predicted_labels = self.discriminator(gen_data, training=True)\n",
    "\n",
    "        # Compute losses\n",
    "        d_loss_value = GAN.discriminator_loss(real_output=d_real_predicted_labels, generated_output=d_fake_predicted_labels)\n",
    "        g_loss_value = GAN.generator_loss(generated_output=d_fake_predicted_labels)\n",
    "\n",
    "    # Now that we have computed the losses, we can compute the gradients (using the tapes)\n",
    "    gradients_of_discriminator = disc_tape.gradient(d_loss_value, self.discriminator.trainable_variables)\n",
    "    gradients_of_generator = gen_tape.gradient(g_loss_value, self.generator.trainable_variables)\n",
    "\n",
    "    # Apply gradients to variables\n",
    "    self.d_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "    self.g_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "\n",
    "    return d_loss_value, g_loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GAN Architectures\n",
    "- Implemented as Keras Sequential Models\n",
    "- Two GANs per paper\n",
    "    - Small version for MNIST handwritten digits with shape (28, 28, 1)\n",
    "    - Larger version for CelebA facial images with shape (256, 256, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def build_generator(self):\n",
    "    noise_shape = (self.get_noise_dim(),)\n",
    "\n",
    "    model = Sequential([\n",
    "\n",
    "        # project and reshape\n",
    "        layers.Dense(7*7*128, use_bias=False, input_shape=noise_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Reshape((7, 7, 128)),\n",
    "        # shape (7, 7, 128)\n",
    "\n",
    "        # stride 2 -> larger image\n",
    "        # thiccness 64 -> channels\n",
    "        layers.Conv2DTranspose(64, (5,5), strides=(2,2), padding='same', use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        # shape (14, 14, 64)\n",
    "\n",
    "        layers.Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', use_bias=False, activation='tanh')\n",
    "        # shape (28, 28, 1)\n",
    "\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def build_discriminator(self):\n",
    "    img_shape = (28, 28, 1)\n",
    "\n",
    "    model = Sequential([\n",
    "\n",
    "        layers.Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=img_shape),\n",
    "        layers.LeakyReLU(alpha=.2),\n",
    "        # shape (14, 14, 64)\n",
    "\n",
    "        layers.Conv2D(128, (5,5), strides=(2,2), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=.2),\n",
    "        # shape (7, 7, 128)\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results (Lercher)\n",
    "Results are generated by the GANs after 50 epochs with:\n",
    "- 60,000 MNIST images \n",
    "- 50,000 faces from CelebA\n",
    "\n",
    "For CelebA the architecture was reduced by half, \n",
    "as the original architecture took too long on my machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DCGAN\n",
    "<div>\n",
    "    <img alt=\"DCGAN MNIST\"\n",
    "         src=\"../gan/models/mnist/dcgan/progress.gif\"\n",
    "         style=\"width: 30%; float: left;\"/>\n",
    "    <img alt=\"DCGAN Faces\"\n",
    "         src=\"../gan/models/faces/dcgan_reduced_architecture/progress.gif\"\n",
    "         style=\"width: 30%; margin-left:20px; float: left;\"/>\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### HR-DCGAN\n",
    "<div>\n",
    "    <img alt=\"HR-DCGAN MNIST\"\n",
    "         src=\"../gan/models/mnist/hr_dcgan/progress.gif\"\n",
    "         style=\"width: 30%; float: left;\"/>\n",
    "    <img alt=\"HR-DCGAN Faces\"\n",
    "         src=\"../gan/models/faces/hr_dcgan_reduced_architecture/progress.gif\"\n",
    "         style=\"width: 30%; margin-left:20px; float: left;\"/>\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparison with Existing Networks\n",
    "Our generator has inferior performance compared to results from the HR-DCGAN paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![HR-DCGAN 150 Epochs](lercher/hrdcgan_150_epochs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![HR-DCGAN 150 Epochs](lercher/hrdcgan_19_epochs_celeba.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Calculation of Fr√©chet Inception Distance\n",
    "Measures the distance of feature distributions between real and generated images (lower is better)\n",
    "\n",
    "$ FID((m,C),(m_w,C_w)) = \\left \\| m-m_w \\right \\|^2_2 + Tr(C + C_w - 2(CC_w)^\\frac{1}{2}) $\n",
    "\n",
    "- $(m,C)$ is mean and covariance of generated images\n",
    "- $(m_w,C_w)$ is mean and covariance of real-world imagesis covariance  \n",
    "- $Tr(.)$ sums up the diagonal elements of the matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_fid(act1, act2):\n",
    "    # calculate mean and covariance statistics\n",
    "    mu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
    "    \n",
    "    # calculate sum squared difference between means\n",
    "    ssdiff = sum((mu1 - mu2)**2.0)\n",
    "    \n",
    "    # calculate sqrt of product between cov\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    if iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "        \n",
    "    # calculate score\n",
    "    fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    \n",
    "    return fid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fr√©chet Inception Distance\n",
    "Features are extracted by Keras' pretrained InceptionV3 model\n",
    "\n",
    "FID results for a sample size of 2,000 images:\n",
    "\n",
    "|          | MNIST | CelebA | \n",
    "|:---------|:------|:------|\n",
    "| DCGAN    | 25.0  | 494.8 |\n",
    "| HR-DCGAN | 27.8  | 565.0 |\n",
    "| HR-DCGAN* | - | 8.44 |\n",
    "\n",
    "*) Stated in the paper's results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training of Original Architectures with CelebA\n",
    "Previously:\n",
    "- Training images were reduced to 50,000\n",
    "- Image size was reduced to (256, 256, 1)\n",
    "- Layers were reduced by half, instead with stride 4\n",
    "\n",
    "\n",
    "New architecture:\n",
    "- All training images where faces are detected are used\n",
    "- Image size set to (512, 512, 3)\n",
    "- All layers were implemented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training Original Architectures with CelebA /2\n",
    "<div>\n",
    "    <img src=\"../gan/models/faces/dcgan/progress.gif\"\n",
    "         style=\"width: 30%; float: left;\"/>\n",
    "    <img src=\"../gan/models/faces/hr_dcgan/progress.gif\"\n",
    "         style=\"width: 30%; margin-left:20px; float: left;\"/>\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Image Generation with our Models\n",
    "Finally, we provide a short script to create images with our trained generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "\n",
    "# Execute this to avoid internal tf error\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact_manual\n",
    "\n",
    "# add the project's base dir to interpreter paths\n",
    "import sys\n",
    "sys.path.insert(1, '../../')\n",
    "from gan import GAN\n",
    "\n",
    "# use this hack as Jupyter Notebook \n",
    "# cannot create new reference in interact_manual\n",
    "gan = {'gan': None} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Generator:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc269c338b24ad7ad0ee8f2f48d12a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='architecture', options=('dcgan', 'hr_dcgan'), value='dcgan'), Drop‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Select Generator:\")\n",
    "@interact_manual\n",
    "def test(architecture=['dcgan', 'hr_dcgan'], dataset=['mnist', 'faces']):\n",
    "    gan['gan'] = GAN.import_gan(path=f\"../gan/models/{dataset}/{architecture}{'_reduced_architecture' if dataset=='faces' else ''}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cede9ed6e0af4861be94177815ccdcf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Run Interact', style=ButtonStyle()), Output()), _dom_classes=('widge‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@interact_manual\n",
    "def generate():\n",
    "    img = gan['gan'].generate()\n",
    "    plt.imshow((img[0, :,:,0]), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "- Results are promising\n",
    "    - Outline of face\n",
    "    - Facial features like eyes, nose, and mouth\n",
    "    - Different hairstyles\n",
    "- Results trained on the server got worse again\n",
    "    - Mode collapse (should be avoided by BatchNorm)\n",
    "    - Adaption of model architecture or training process\n",
    "        - Larger number of filters\n",
    "        - Longer training for single adversary\n",
    "        - Different hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7. Conclusion\n",
    "Overall, our results after just 50 epochs look promising especially for the DCGAN. The center of generated images already contains the outline of a round face including eyes, nose and mouth. Even hair is already generated in various hairstyles. \n",
    "\n",
    "The results from our normal DCGAN look better than from our HR-DCGAN. \n",
    "I assume this has to do with the size of training images, as 256x256x1 is not quite high-resolution. In their paper, the authors used training images of size 512x512x3 [4] which I regarded as similar size, as this would only add a single convolution layer multiplying or dividing our current resolution by 2x2 again.\n",
    "\n",
    "In future work, we would like to train the GAN on a server with the original network architectures and the whole range of 200,000 celebrity faces. This should give more variety and allows the discriminator to learn when a face is _consistent_ because current faces contain a mixture of real faces, e. g. different hairstyles or two different eyes.\n",
    "Then, we could also add color channels which increases the size per image by an additional factor of 3.\n",
    "-> results are still bad, as mode collapse happened "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('venv': venv)",
   "language": "python",
   "name": "python37664bitvenvvenv49e7057a7de4479cb755d63b248db7b0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
